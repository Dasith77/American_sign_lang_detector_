# American_sign_lang_detector_
![ACTUAL DESIGN - This is what we publish!](https://github.com/Dasith77/American_sign_lang_detector_/assets/65776391/6097b390-bb16-459e-a791-184a10cea026)


# Overall Project description

This is semester 5 embedded system engineering project based on machine learning and computer vision to detedct hand sign languages and convert it  to a audible sounds.

The American Sign Language Interpreter is a powerful application designed to bridge communication gaps between hearing-impaired individuals and the broader community. Leveraging the capabilities of the MediaPipe framework, this interpreter utilizes landmark detection technology to accurately recognize and translate American Sign Language gestures into textual or auditory output. Unlike traditional pixel recognition methods, which often struggle with varying lighting conditions and complex hand movements, the MediaPipe-based approach excels in robustly identifying key landmarks on hands, making it more reliable and adaptable. This enables the interpreter to provide more accurate and real-time translations, enhancing accessibility and fostering effective communication between the deaf and hearing communities. Whether in educational settings, public spaces, or everyday interactions, the American Sign Language Interpreter powered by MediaPipe serves as an essential tool for promoting inclusivity and understanding.

# I implement this project as 2 stages.

    1. Impelemt machine learning part to recognizes and build a sentence using those characters.
    2. Further implementation into a web based application.
    


# 1 . First stage

## 1.1 Collecting dataset from webcam

